\documentclass[DM,lsstdraft,STS,toc]{lsstdoc}
\usepackage{enumitem}
\input meta.tex

\begin{document}

\def\product{Qserv}

\setDocCompact{true}

\title[STS for \product]{\product{} Software Test Specification}

\author{Fritz Mueller}
\setDocRef{\lsstDocType-\lsstDocNum}
\date{\vcsdate}

\setDocAbstract{
This document describes the detailed test specification for the \product{}.
}

\setDocChangeRecord{%
    \addtohist{1}{\vcsdate}{Initial release.}{FM}
}

\maketitle

\section{Introduction}
\label{sec:intro}

This document specifies the test procedure for \product{}. \product{} is a distributed
shared-nothing RDBMS which will host LSST catalogs. 

\subsection{Objectives}
\label{sec:objectives}

This document builds on the description of LSST Data Management's approach to
testing as described in \citeds{LDM-503} to describe the detailed tests that
will be performed on the \product{} as part of the verification of the DM system.

It identifies test designs, test cases and procedures for the tests, and the
pass/fail criteria for each test.

\subsection{Scope}
\label{sec:scope}

This document describes the test procedures for the following components of
the LSST system (as described in \citeds{LDM-148}):

\begin{itemize}
  \item{Parallel Distributed Database (Qserv)}
\end{itemize}

\subsection{Applicable Documents}
\label{sec:docs}

\addtocounter{table}{-1}

\begin{tabular}[htb]{l l}
  \citeds{LDM-135} & LSST Qserv Database Design \\
  \citeds{LDM-294} & LSST DM Organization \& Management \\
  \citeds{LDM-502} & The Measurement and Verification of DM Key Performance Metrics \\
  \citeds{LDM-503} & LSST DM Test Plan \\
  \citeds{LDM-555} & LSST DM Database Requirements \\
\end{tabular}

\subsection{References}
\label{sec:references
}
\renewcommand{\refname}{}
\bibliography{lsst,refs,books,refs_ads}

\newpage
\section{Approach}
\label{sec:approach}

The approaches taken for the tests described here are:

\begin{itemize}

  \item{Ongoing inspection of design documents, code, and CI system logs to verify that \product{} design 
  and implementation meet DM software quality standards in general, and requirements as expressed in 
  \citeds{LDM-555} in particular;}

  \item{Ongoing deployment and continuous operation of \product{} in a Prototype Data Access Center
  (PDAC) in order to assess basic reliability, fitness for purpose, and integration with adjacent 
  subsystems;}

  \item{Annual deployment of \product{} to test clusters, followed by synthesis and ingestion
  of test datasets and scripted performance/load/stress testing. The cluster size/capabilities and the
  scale of the synthetic test dataset are both evolved along a path toward anticipated LSST operational
  scale.}

\end{itemize} 

\subsection{Tasks and criteria}
\label{sec:tasks}

\product{} is a containerized, distributed, Linux application, which is deployed on machine clusters.
At the scales to be tested, these clusters are comprised of one to several head ("czar") nodes and
additionally on the order of tens to hundreds of shard ("worker") nodes, interconnected locally via a
high-performance network. Head and shard nodes are provisioned each with on the order of 10s of gigabytes of
RAM, and each with on the order of 10s of terabytes of locally attached storage.

Ongoing deployment, continuous operation, and integration tests are carried out on machines within the
Prototype Data Access Center (PDAC), a dedicated machine cluster physically located at NCSA's National 
Peta-scale Compute Facility, maintained by NCSA staff. Catalog datasets which are maintained within
the PDAC Qserv instance and which are used for this testing include, simultaneously:

\begin{itemize}
  \item{An LSST stack reprocessed version of the SDSS Stripe 82 catalog ({\textasciitilde{}}10 TB);}
  \item{IRSA AllWISE and NEOWISE catalogs ({\textasciitilde{}}50 TB);}
  \item{An LSST stack reprocessed version of the HSC catalog (scheduled; {\textasciitilde{}}50 TB).}
\end{itemize}

Tasks required for these tests include periodic update of the software deployed on the PDAC,
periodic ingest of additional test datasets, and inter-operation with adjacent subsystems.  Uptime
is monitored cumulatively throughout these activities to gain quantitative insight into system
stability and reliability.

Scaling, load, and stress testing are carried out on an additional machine cluster located
CC-IN2P3 in Lyon, maintained by CC-IN2P3 staff. Scaling tests are run annually, by issuing
a representative mix of concurrent queries against a synthetic catalog while monitoring
average query execution times per query type.  The scaling test dataset size and query 
concurrency level are increased each year on a glide path toward the full scale of Data Release 1.

Tasks required for these tests include generation and ingest of each successive test dataset, and
execution of scripts which issue and monitor the suites of representative test queries.

\subsection{Features to be tested}
\label{sec:feat2test}

This version of the \product{} test specification addresses only basic product verification, basic
reliability, and performance/scale testing -- a bare minimum required to conduct ongoing development 
and verify that \product{} remains on a realistic path towards meeting its most technically challenging
requirements: those related to successful operability at the scale that will be required by LSST.

\subsection{Features not to be tested}
\label{sec:featnot2test}

Testing of the following are NOT YET COVERED in this specification:

\begin{itemize}
  \item{Fault-tolerance and disaster recovery;}
  \item{Schema evolution;}
  \item{Data ingest performance;}
  \item{Query reproducibility;}
  \item{Cross-match with external datasets.}
\end{itemize}

It is anticipated that test specifications and cases for all of the above will be developed
and added to future revisions of this document. 

\subsection{Pass/fail criteria}
\label{sec:passfail}

The results of all tests will be assessed using the criteria described in \citeds{LDM-503} \S4.

\subsection{Suspension criteria and resumption requirements}
\label{suspension}

Refer to individual test cases where applicable.

\subsection{Naming convention}

All tests are named according to the pattern \textsc{prod-scope-xx-yy} where:

\begin{description}[font=\normalfont\scshape]

  \item[prod]{The product code, per \citeds{LDM-294}. Relevant entries for this document are:
    \begin{description}[font=\normalfont\scshape,topsep=-1.0ex]
      \item[qserv]{Qserv distributed database system}
    \end{description}
  }

  \item[scope]{The type of test being described:
    \begin{description}[font=\normalfont\scshape,topsep=-1.0ex]
      \item[acp]{concerning acceptance testing}
      \item[bck]{concerning backup and restore testing}
      \item[fun]{concerning functional testing}
      \item[ins]{concerning installation testing}
      \item[int]{concerning integration testing}
      \item[itf]{concerning interface testing}
      \item[mnt]{concerning maintenance testing}
      \item[prf]{concerning performance testing}
      \item[reg]{concerning regression testing}
      \item[ver]{concerning verification testing}
    \end{description}
  }

  \item[xx]{Test design number (in increments of 10)}
  \item[yy]{Test case number (in increments of 5)}

\end{description}

\input{specs/specs.tex}
\input{cases/cases.tex}
\input{procedures/procedures.tex}

\end{document}
